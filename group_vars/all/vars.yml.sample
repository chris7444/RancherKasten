###
# Copyright (2020) Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License");
# You may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
###
---

rancher_subnet: 16.78.0.0/19                              # subnet to use on the 'vm_portgroup' VLAN
gateway: '16.78.0.1'                                      # gateway for the above subnet (see your net admin)
ntp_servers: ['16.110.135.123']                           # List of NTP servers
dns_servers: ['16.78.15.31']                              # list of DNS servers
dns_suffixes: ['k8s.org','flab.local']                    # list of DNS suffixes
domain_name: k8s.org

#
# DHCP related settings, this github branch uses network profiles
#
#dhcp_subnet: 10.5.61.0/24                                # subnet to use on the above VLAN (see your net admin)
#dhcp_range: '10.5.61.40 10.5.61.100'                     # DHCP range to use on the above VLAN 
#dhcp_default_lease_time: 28800                           # DHCP default lease time (8 hours)
#dhcp_max_lease_time: 57600                               # DHCP maximum lease time (16 hours)

#
# vcenter related settings
#
vcenter_hostname: vcenter7c.flab.local
vcenter_username: Administrator@vsphere.local             # Admin user for your vCenter environment
vcenter_password: "{{ vault_vcenter_password }}"          # Encrypted in group_vars/all/vault.yml
vcenter_validate_certs: false                             # true not implemented/tested
vcenter_cluster: dhci1                                    # Name of your SimpliVity Cluster (must exist)
vm_portgroup: DCN                                         # portgroup that the VMS connect to (must exist)
datacenter: DHCI                                          # Name of your DATACENTER (must exist)
datastore: vmfs2                                          # Datastore where the VMs are landed (must exist)
datastore_size: 1024                                      # size in GiB of the VM datastore
cluster_name: rke                                         # Name of the K8S Cluster. A VM folder with the same name is created if needed

#
# folders, templates and OVAs, templates are created using the corresponding OVA if they cannot be found (and only if they cannot be found)
#
admin_folder: RancherInfra                                # Folder and pool name for Rancher Cluster VMs and  Templates
#packer_os_flavor: ubuntu                                 # OS flavor of the template, can be ubuntu (default) or centos. case sensitive
#
# Public key to use for login in the rancher nodes (the VM hosting the Rancher Cluster)
#
ssh_key: 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDArNokIdAdYTP735ZJLhnbkhmHuRUQLZczLg8xX6AOFwiSEPvF7NX4FOTXHXJ/3T6QT94xl27i/F1hq+zJ110nWQlAAngWrtZpcwMdAL5kr5+M5WPOScH+v+xPy0pB5MC235h2YDGM9t/uh3IJ481Du53HgEJgdezX9p26LT/LcUIQHvjNFLYzDyp70M+K29vT+P/tKXOKIVIWf855feg6wEihQhId/8QvWQBK0FjkrDJGQbK79igOriltZzcI2/3XK0d556raOZqYxa/cVXT1Jd/QoniQurpDboC3x5EPJ3+EzTRl9OwukojZwqFnnWGIZL4/RDgA/FokuFUdryRjeodi4jdRXGCwoXKbL25fvH1ekxJzo+5GKXjT4HFPYb++bPjlcVaJP8UObAGGsLjFf3uikISH0e+8N1CCG+wwoeXtrR1FDL6iR1/+kWy5T95oK3JJAlQhzztWn7jlw2CB/OrAw3vLKhHWEKP/8DbeSQNNJ9PanyH+gE8AG8AR9pU= chris@ubuntu-01' 

proxy:
  http:   "http://web-proxy.houston.hpecorp.net:8080/"     #  http:  "http://user:password@10.12.7.21:8080/"
  https:  "http://web-proxy.houston.hpecorp.net:8080/"
  except: "127.0.0.1,localhost,10.0.0.0/8,.k8s.org,.flab.local,.svc,.cluster.local"

#
# Kubernetes cluster hosting the Rancher Server app
#
rke_cluster:
  docker_url: https://releases.rancher.com/install-docker/19.03.sh      # 20.10 not supported by rke 1.2.5
  kubernetesVersion: v1.18.15-rancher1-1                                # unresolved issues with v1.20 for now

#
# Rancher Server itself
#
rancher:
  url: https://rancher.k8s.org    # this the FQDN at which Rancher Server can be reached
  hostname: rancher.k8s.org       # generally same fqdn as the one in the url above but not necessarily
#  channel: stable                # latest, stable or alpha, default 'stable'
#  version: x.y.z.                # version of rancher server. defaults to 'latest' in rancher-stable, last tested was 2.5.7
  validate_certs: False           #
  apiversion: v3                  # Playbooks designed for v3 of the API
  engineInstallURL: 'https://releases.rancher.com/install-docker/20.10.sh'     # or 19.03.sh
# If user supplied certificate wanted
#  tls_source: secret                               # either rancher or secret, letsEncrypt unsupported (See doc), rancher is the default
#  tls_privateCA: true                              # if using a private root CA, default is false (ie you use a public root CA)
#  tls_cacert_file: /home/alan/certs/cacerts.pem    # file containing the root CA certificate. Relevant if tls_privateCA is true
#  tls_certchain_file: /home/alan/certs/cachain.pem # file containing the server certificate followed by the intermediate CA certs (if any)
#  tls_certkey_file: /home/alan/certs/tlskey.pem    # file containing the private key for the Rancher server

#
# usr_tag is a commodity, by configuring the env variable USER_CLUSTER_TAG you can easily
#    re-run the playbook user.yml and deploy another user cluster in its own VM folder and resource pool
#
usr_tag: "{% if lookup('env', 'USER_CLUSTER_TAG') == '' %}dev{% else %}{{ lookup('env','USER_CLUSTER_TAG') }}{% endif %}"

#
# user cluster
#
user_cluster:
  name: "{{ usr_tag }}"                               # name of the user cluster
# vm_template: kas-iscsi2-tpl                         # the default VM template is the admin template
  kubernetesVersion: v1.20.4-rancher1-1               # last version I deployed successfully
  iscsi1_portgroup: iSCSI2                            # if defined user VMs are connected to this portgroup
  csi:
    driver: nimble                                    # nimble, vsphere or none
    vsphere:                                          # relevant if driver is vsphere
      datastore_name: cnstests                        # must exist
      storageclass_name: vsphere
      datastore_size: 20
      is_default_class: "true"
    nimble:                                           # relevant if driver is nimble
      storageclass_name: nimble
      nimble_group: "16.78.21.159"                    # management IP address of the Nimble array (or group)
      user: admin                                     # password is stored in vault.yml
      is_default_class: "true"
      nimble_folder: "{{ usr_tag }}"                  # you must create the folder in Nimble before running the playbook
  vcenter_credsname: mycreds                          # only one vCenter cluster supported at this time
  vcenter_folder: "{{ usr_tag }}Cluster"              # folder and pool name for the user cluster VMs
  kasten_nfs_server: 16.78.15.15                      # IP address of the NFS server
  kasten_nfs_share_path: "/nas/kasten-nfs"            # name of NFS share where Kasten can export backup data
  pools:
   - name: "{{ usr_tag }}-master-pool"
     etcd: true
     master: true
     worker: false
     count: 1
     hostPrefix: "{{ usr_tag }}-mas"
     node_template:
       name: "{{ usr_tag }}-master-node"
       cpu_count: 2
       disk_size: 20000
       memory_size: 8192
   - name: "{{ usr_tag }}-worker-pool"
     etcd: false
     master: false
     worker: true
     count: 1
     hostPrefix: "{{ usr_tag }}-wrk"
     node_template:
       name: "{{ usr_tag }}-worker-node"
       cpu_count: 4
       disk_size: 40000
       memory_size: 8192

#
# Active Directory Integration
#
#ad_ca_file: "path to your AD CA certificate in pem format"                # A default file is provided in playbooks/roles/ad-auth/files/ca.pem
ad_login_domain: AM2                                                      # Name of the AD Domain
ad_server_name: mars-adds.am2.cloudra.local                               # Name of the AD Server
ad_service_account_username: adreader                                     # AD service account username
ad_service_account_password: "{{ vault_ad_service_account_password }}"    # AD service account password
ad_tls: true                                                              # Use TLS for AD
ad_port: 636                                                              # Port number to access AD service
ad_group_search_base: ""                                                  # Search base string used for group lookups
ad_group_search_filter: ""                                                # Search filter for group lookups
ad_user_search_base: "cn=Users,dc=am2,dc=cloudra,dc=local"                # Search base string used for user lookups
ad_user_search_filter: ""                                                 # Search filer for user lookups

#
# Loadbalancer Variables
#
loadbalancers:
  backend:
    vip: 16.78.21.180/19
    vrrp_router_id: 81
    nginx_max_fails: 1
    nginx_fail_timeout: 10s
    nginx_proxy_timeout: 10m
    nginx_proxy_connect_timeout: 60s

registry:
#  tls_cacert_file: /home/alan/certs/registry/ca.crt
#  tls_certchain_file: /home/alan/certs/registry/registry-cachain.pem
#  tls_certkey_file: /home/alan/certs/registry/registry-key.pem
  fqdn: registry.k8s.org
  projects:
  - project_name:  nginxplus
    is_public: false
    content_trust: false
    prevent_vul: true
    severity: high
    auto_scan: true


# If you bring your own certs for the registry,
#  1- define additional_cas as an array of file paths (see below)
#  2- add an entry in the array specifying the path to the root CA certificate. This shouild be the same as registry.tls_cacert_file
#  3- you may add additional CA certificates if needed
#additional_cas:
# - /home/alan/certs/registry/ca.crt


kasten_location:
  name: hpe-scality
  endpoint: https://scality1-endpoint.flab.local
  region: ""
  bucket: kasten
  skipSSLVerify: true
  secret:
    name: hpe-scality-secret
    access_key: "{{ vault_s3_access_key }}"
    secret_key: "{{ vault_s3_secret_key }}"


delete_templates: false
